{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "32445566-7dda-4f93-96dd-674062afdf25",
      "metadata": {
        "id": "32445566-7dda-4f93-96dd-674062afdf25"
      },
      "source": [
        "# CPE494/663 GENERATIVE ARTIFICIAL INTELLIGENCE\n",
        "## Lab 1: PyTorch Discriminative Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68455b1b-288e-487a-b13b-5d752e86ab23",
      "metadata": {
        "id": "68455b1b-288e-487a-b13b-5d752e86ab23"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "161135b4-2c46-4550-932e-36b323c87719",
      "metadata": {
        "id": "161135b4-2c46-4550-932e-36b323c87719"
      },
      "source": [
        "Using `RealWaste` dataset from https://archive.ics.uci.edu/dataset/908/realwaste  \n",
        "The dataset contains 4752 images of waste items across 9 material types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8477e2c4-a796-4974-9232-d07980843162",
      "metadata": {
        "id": "8477e2c4-a796-4974-9232-d07980843162"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ecff59-041b-48c4-82b2-76a7ae8b1d7a",
      "metadata": {
        "id": "c2ecff59-041b-48c4-82b2-76a7ae8b1d7a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if not Path(\"realwaste-main\").is_dir():\n",
        "    print(\"Downloading dataset...\")\n",
        "    resp = requests.get(\"https://archive.ics.uci.edu/static/public/908/realwaste.zip\")\n",
        "    content = BytesIO(resp.content)\n",
        "    with zipfile.ZipFile(content, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"Downloaded and extracted dataset to `./realwaste-main`\")\n",
        "else:\n",
        "    print(\"Dataset already exists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c609d610-3f8f-4b4e-a809-eb700d51a480",
      "metadata": {
        "id": "c609d610-3f8f-4b4e-a809-eb700d51a480"
      },
      "source": [
        "### Define `datasets` and `DataLoader`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f66f30fe-cb54-4802-af73-76972a9e4636",
      "metadata": {
        "id": "f66f30fe-cb54-4802-af73-76972a9e4636"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms, datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_seed(42)"
      ],
      "metadata": {
        "id": "iZUsoh-L683v"
      },
      "id": "iZUsoh-L683v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6107e4f-f28b-4d0d-9abd-2fefb101116a",
      "metadata": {
        "id": "c6107e4f-f28b-4d0d-9abd-2fefb101116a"
      },
      "outputs": [],
      "source": [
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea45b106-c6ed-4602-b292-110a4b50d2ca",
      "metadata": {
        "id": "ea45b106-c6ed-4602-b292-110a4b50d2ca"
      },
      "outputs": [],
      "source": [
        "# Define data transformation\n",
        "data_transforms = transforms.Compose([transforms.Resize((256, 256)),\n",
        "                                      transforms.ToTensor()\n",
        "                                     ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86443e1-29ff-4b42-b0e1-a4706ea9d389",
      "metadata": {
        "id": "c86443e1-29ff-4b42-b0e1-a4706ea9d389"
      },
      "outputs": [],
      "source": [
        "# Read image directory as dataset\n",
        "realwaste_img = datasets.ImageFolder(\"realwaste-main/RealWaste/\",\n",
        "                                     transform=data_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c5fcf79-a045-4c67-8a82-7514b009ad4d",
      "metadata": {
        "id": "9c5fcf79-a045-4c67-8a82-7514b009ad4d"
      },
      "outputs": [],
      "source": [
        "# Get number of classes in the dataset\n",
        "num_classes = len(realwaste_img.classes)\n",
        "print(f\"Dataset contains {num_classes} classes: {realwaste_img.classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f9097f-2c75-41e0-8043-0bb9827b96db",
      "metadata": {
        "id": "37f9097f-2c75-41e0-8043-0bb9827b96db"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training and test set\n",
        "train_ratio = 0.7\n",
        "\n",
        "train_size = int(train_ratio * len(realwaste_img))\n",
        "test_size = len(realwaste_img) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(realwaste_img,\n",
        "                                           [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a124f9-984f-4544-9d7f-3dba21c8c264",
      "metadata": {
        "id": "f8a124f9-984f-4544-9d7f-3dba21c8c264"
      },
      "outputs": [],
      "source": [
        "# Show sample image from training set\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def show_image(img_tensor):\n",
        "    img = img_tensor.numpy().transpose((1, 2, 0))\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "show_image(images[0])\n",
        "print(f\"Label: {realwaste_img.classes[labels[0]]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c81fd260-2538-4039-a040-28baa36e6ffe",
      "metadata": {
        "id": "c81fd260-2538-4039-a040-28baa36e6ffe"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2f0e18-0c85-40c2-b4a2-4d8ab53e9570",
      "metadata": {
        "id": "af2f0e18-0c85-40c2-b4a2-4d8ab53e9570"
      },
      "source": [
        "**Your turn:** Develop and train a discriminative neural network to classify images within the `RealWaste` dataset.  \n",
        "You can use any neural network architecture; however, you must provide a technical justification for your design choices.  \n",
        "Evaluate the model's performance using appropriate loss functions and metrics.  \n",
        "\n",
        "**What to submit:** 2 files:\n",
        "1. A Python notebook (.ipynb)\n",
        "2. Export your notebook as a PDF file\n",
        "   \n",
        "Submit to `Lab 1 - PyTorch` activity in LEB2.  \n",
        "\n",
        "\n",
        "**Due date:** 26/01/2026 17:59"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "602c2141-1ca1-41fe-8acc-0dc38ffab10b",
      "metadata": {
        "id": "602c2141-1ca1-41fe-8acc-0dc38ffab10b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Helper Functions for Scaling\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def make_divisible(v, divisor=8, min_value=None):\n",
        "    \"\"\"\n",
        "    Ensures all layer channels are divisible by 8 (friendly to hardware).\n",
        "    Adapted from the original EfficientNet implementation.\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "def get_compound_params(phi):\n",
        "    \"\"\"\n",
        "    Calculates the depth, width, and resolution multipliers\n",
        "    based on the compound coefficient phi.\n",
        "\n",
        "    EfficientNet Constants:\n",
        "    alpha = 1.2 (depth)\n",
        "    beta  = 1.1 (width)\n",
        "    gamma = 1.15 (resolution)\n",
        "    \"\"\"\n",
        "    alpha, beta, gamma = 1.2, 1.1, 1.15\n",
        "\n",
        "    depth_mult = alpha ** phi\n",
        "    width_mult = beta ** phi\n",
        "    res_mult   = gamma ** phi\n",
        "\n",
        "    return depth_mult, width_mult, res_mult\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Standard ResNet Components (Bottleneck)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledResNet(nn.Module):\n",
        "    def __init__(self, block, layers, width_mult=1.0, num_classes=1000,\n",
        "                 zero_init_residual=False, groups=1, width_per_group=64):\n",
        "        super(ScaledResNet, self).__init__()\n",
        "\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        # Base settings\n",
        "        self.inplanes = make_divisible(64 * width_mult)\n",
        "        self.base_width = width_per_group\n",
        "        self.groups = groups\n",
        "\n",
        "        # Initial Stem (Scale the stem width too)\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Calculate scaled channel widths for the 4 stages\n",
        "        # Base ResNet planes are [64, 128, 256, 512]\n",
        "        layer_planes = [\n",
        "            make_divisible(64 * width_mult),\n",
        "            make_divisible(128 * width_mult),\n",
        "            make_divisible(256 * width_mult),\n",
        "            make_divisible(512 * width_mult)\n",
        "        ]\n",
        "\n",
        "        # Create layers\n",
        "        self.layer1 = self._make_layer(block, layer_planes[0], layers[0])\n",
        "        self.layer2 = self._make_layer(block, layer_planes[1], layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, layer_planes[2], layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, layer_planes[3], layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Final fully connected layer\n",
        "        final_channels = layer_planes[3] * block.expansion\n",
        "        self.fc = nn.Linear(final_channels, num_classes)\n",
        "\n",
        "        # Weight Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "\n",
        "        # If stride != 1 or input channels != output channels, we need a downsample projection\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, norm_layer=norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "p_TZJ67a5zMd"
      },
      "id": "p_TZJ67a5zMd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 4. Builder Function\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def efficient_scaled_resnet50(phi=0, num_classes=1000):\n",
        "    \"\"\"\n",
        "    Constructs a ResNet-50 model scaled using EfficientNet compound scaling.\n",
        "\n",
        "    Args:\n",
        "        phi (int): The compound scaling coefficient (e.g., 0 for base, 1, 2...).\n",
        "                   phi=0 is standard ResNet50.\n",
        "    \"\"\"\n",
        "    # 1. Get multipliers\n",
        "    d_mult, w_mult, r_mult = get_compound_params(phi)\n",
        "\n",
        "    # 2. Scale Depth (ResNet50 base layers: [3, 4, 6, 3])\n",
        "    base_layers = [3, 4, 6, 3]\n",
        "    # We use ceiling to round up layers, ensuring we don't lose depth on small scales\n",
        "    scaled_layers = [int(math.ceil(l * d_mult)) for l in base_layers]\n",
        "\n",
        "    # 3. Scale Resolution (Target Input Size)\n",
        "    base_res = 224\n",
        "    target_res = int(base_res * r_mult)\n",
        "\n",
        "    print(f\"--- Efficient Scaled ResNet50 (Phi={phi}) ---\")\n",
        "    print(f\"Depth Multiplier: {d_mult:.2f} -> Layers: {scaled_layers}\")\n",
        "    print(f\"Width Multiplier: {w_mult:.2f}\")\n",
        "    print(f\"Target Resolution: {target_res}x{target_res}\")\n",
        "\n",
        "    # 4. Build Model\n",
        "    model = ScaledResNet(\n",
        "        block=Bottleneck,\n",
        "        layers=scaled_layers,\n",
        "        width_mult=w_mult,\n",
        "        num_classes=num_classes\n",
        "    )\n",
        "\n",
        "    return model, target_res"
      ],
      "metadata": {
        "id": "5ES3nGmZ5xUL"
      },
      "id": "5ES3nGmZ5xUL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Create a Phi=1 scaled ResNet50 (roughly \"ResNet-50-B1\")\n",
        "model, resolution = efficient_scaled_resnet50(phi=1, num_classes=num_classes)\n",
        "model.to(device)\n",
        "\n",
        "# Test with a dummy input of the calculated resolution\n",
        "dummy_input = torch.randn(1, 3, resolution, resolution)\n",
        "output = model(dummy_input)\n",
        "\n",
        "print(f\"Output shape: {output.shape}\") # Should be [1, 1000]"
      ],
      "metadata": {
        "id": "gvnUtW-b5tjs"
      },
      "id": "gvnUtW-b5tjs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c5325b3"
      },
      "source": [
        "### Training and Evaluation"
      ],
      "id": "8c5325b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "486545d5"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 5\n",
        "\n",
        "print(f\"Training on: {device}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f'Epoch {epoch + 1}, Training Loss: {running_loss / len(train_loader):.3f}, Training Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Evaluation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "        for data in test_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    print(f'Epoch {epoch + 1}, Test Loss: {test_loss / len(test_loader):.3f}, Test Accuracy: {test_accuracy:.2f}%\\n')\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "id": "486545d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "594d1c9a-f93f-4d0c-a72c-511ed612c0ef",
      "metadata": {
        "id": "594d1c9a-f93f-4d0c-a72c-511ed612c0ef"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}